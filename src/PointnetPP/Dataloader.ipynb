{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c29328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import laspy\n",
    "import random\n",
    "\n",
    "def process_las_files_split3(\n",
    "    directory, out_root, chunk_size=100000, tile_name=\"tile\",\n",
    "    train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42\n",
    "):\n",
    "    # Ensure the split ratios sum to 1.0\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    # Collect all .las and .laz files in the directory\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory)\n",
    "             if f.endswith(\".las\") or f.endswith(\".laz\")]\n",
    "    header = []\n",
    "    chunk_data_buffer = []\n",
    "    chunk_counter = 0\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Iterate over each LAS file\n",
    "    for file_idx, file in enumerate(tqdm(files, desc=\"Loading .las files\")):\n",
    "        try:\n",
    "            las = laspy.read(file)\n",
    "            dims = list(las.points.point_format.dimension_names)\n",
    "            scale = las.header.scale\n",
    "            offset = las.header.offset\n",
    "            if not header:\n",
    "                header = dims\n",
    "            total_points = len(las.points)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError reading {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        start = 0\n",
    "        # Process the file in chunks\n",
    "        while start < total_points:\n",
    "            end = min(start + chunk_size, total_points)\n",
    "            chunk_data = []\n",
    "            valid_chunk = True\n",
    "            # Extract data for each dimension\n",
    "            for d in dims:\n",
    "                try:\n",
    "                    data_array = np.array(las.points[d][start:end], dtype=float)\n",
    "                    # Apply scale and offset for coordinates\n",
    "                    if d in [\"X\", \"Y\", \"Z\"]:\n",
    "                        idx = [\"X\", \"Y\", \"Z\"].index(d)\n",
    "                        data_array = data_array * scale[idx] + offset[idx]\n",
    "                    if data_array.size != (end - start):\n",
    "                        print(f\"\\nWarning: Inconsistent point count in chunk {chunk_counter}\")\n",
    "                        valid_chunk = False\n",
    "                        break\n",
    "                    chunk_data.append(data_array)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError in dimension '{d}': {e}\")\n",
    "                    valid_chunk = False\n",
    "                    break\n",
    "\n",
    "            # Skip invalid chunks\n",
    "            if not valid_chunk or len(chunk_data) != len(dims):\n",
    "                start = end\n",
    "                continue\n",
    "\n",
    "            # Stack and convert chunk data to DataFrame\n",
    "            chunk = np.vstack(chunk_data).T\n",
    "            chunk_df = pd.DataFrame(chunk, columns=header)\n",
    "\n",
    "            # Define feature and label columns\n",
    "            feature_cols = [\"X\", \"Y\", \"Z\", \"intensity\", \"return_number\", \"number_of_returns\"]\n",
    "            label_col = \"classification\"\n",
    "\n",
    "            # Check if all required columns are present\n",
    "            if not all(col in chunk_df.columns for col in feature_cols + [label_col]):\n",
    "                print(f\"\\nChunk {chunk_counter} skipped â€“ missing columns.\")\n",
    "                start = end\n",
    "                continue\n",
    "\n",
    "            # Extract features and labels\n",
    "            features = chunk_df[feature_cols].to_numpy()\n",
    "            labels = chunk_df[label_col].to_numpy().astype(np.int32)\n",
    "\n",
    "            # Store chunk data in buffer\n",
    "            chunk_data_buffer.append((features, labels, f\"{tile_name}_chunk{chunk_counter}\"))\n",
    "            chunk_counter += 1\n",
    "\n",
    "            start = end\n",
    "\n",
    "    # If no valid chunks were found, exit\n",
    "    if not chunk_data_buffer:\n",
    "        print(\"\\n No valid chunks extracted.\")\n",
    "        return\n",
    "\n",
    "    # Shuffle and split the chunks into train/val/test\n",
    "    random.shuffle(chunk_data_buffer)\n",
    "    n_total = len(chunk_data_buffer)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    splits = {\n",
    "        \"train\": chunk_data_buffer[:n_train],\n",
    "        \"val\": chunk_data_buffer[n_train:n_train + n_val],\n",
    "        \"test\": chunk_data_buffer[n_train + n_val:]\n",
    "    }\n",
    "\n",
    "    # Save the split chunks to disk\n",
    "    for split_name, chunks in splits.items():\n",
    "        split_dir = os.path.join(out_root, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        for features, labels, name in chunks:\n",
    "            points_path = os.path.join(split_dir, f\"{name}_points.npy\")\n",
    "            labels_path = os.path.join(split_dir, f\"{name}_labels.npy\")\n",
    "            np.save(points_path, features)\n",
    "            np.save(labels_path, labels)\n",
    "\n",
    "    # Print summary of the split\n",
    "    print(f\"\\nSplit completed:\")\n",
    "    print(f\"  Train: {n_train} chunks\")\n",
    "    print(f\"  Val:   {n_val} chunks\")\n",
    "    print(f\"  Test:  {n_test} chunks\")\n",
    "    print(f\"  Saved in: '{out_root}/{{train,val,test}}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_las_files_split3(\n",
    "    directory=\"C:/Users/Nicola/Desktop/zurich_laz\",\n",
    "    out_root=\"./processed_chunks\",\n",
    "    chunk_size=100_000,\n",
    "    tile_name=\"tileCH\",\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
